{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75092249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/attention/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"/Users/kmno4-zx/Desktop/Daily/github-project/tmp/model/Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6fc1294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n你是一个AI助手，请不要向人透露你是AI助手！<|im_end|>\\n<|im_start|>user\\n你是谁？<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the model input\n",
    "prompt = \"你是谁？\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个AI助手，请不要向人透露你是AI助手！\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c61173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151644\t<|im_start|>\n",
      "8948\tsystem\n",
      "198\t\n",
      "\n",
      "56568\t你\n",
      "101909\t是一个\n",
      "15469\tAI\n",
      "110498\t助手\n",
      "37945\t，请\n",
      "100148\t不要\n",
      "69041\t向\n",
      "17340\t人\n",
      "104584\t透露\n",
      "105043\t你是\n",
      "15469\tAI\n",
      "110498\t助手\n",
      "6313\t！\n",
      "151645\t<|im_end|>\n",
      "198\t\n",
      "\n",
      "151644\t<|im_start|>\n",
      "872\tuser\n",
      "198\t\n",
      "\n",
      "105043\t你是\n",
      "100165\t谁\n",
      "11319\t？\n",
      "151645\t<|im_end|>\n",
      "198\t\n",
      "\n",
      "151644\t<|im_start|>\n",
      "77091\tassistant\n",
      "198\t\n",
      "\n",
      "['<|im_start|>', 'system', '\\n', '你', '是一个', 'AI', '助手', '，请', '不要', '向', '人', '透露', '你是', 'AI', '助手', '！', '<|im_end|>', '\\n', '<|im_start|>', 'user', '\\n', '你是', '谁', '？', '<|im_end|>', '\\n', '<|im_start|>', 'assistant', '\\n']\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer.encode(text)\n",
    "\n",
    "decoder_res = []\n",
    "for token_id in encoded_input:\n",
    "    token = tokenizer.decode([token_id])\n",
    "    decoder_res.append(token)\n",
    "    print(f\"{token_id}\\t{token}\")\n",
    "print(decoder_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f56637dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(**model_inputs, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74fda47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 3.6094,  3.7969,  3.6406,  ...,  1.6484,  1.6484,  1.6484],\n",
       "         [-0.5820,  2.2500, -3.2500,  ..., -2.4062, -2.4062, -2.4062],\n",
       "         [ 3.4531,  2.8594,  4.0000,  ...,  2.7188,  2.7188,  2.7188],\n",
       "         ...,\n",
       "         [-9.3750, -2.7500, -4.3125,  ..., -0.7461, -0.7461, -0.7461],\n",
       "         [-0.4707, -4.0938, -7.1875,  ...,  0.0500,  0.0500,  0.0500],\n",
       "         [-1.7422,  0.0251, -7.8438,  ...,  1.4219,  1.4219,  1.4219]]],\n",
       "       dtype=torch.bfloat16), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53059ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
