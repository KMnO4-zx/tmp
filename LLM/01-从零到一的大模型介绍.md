# å¤§æ¨¡å‹ä»‹ç»ä¸éƒ¨ç½²

## ä»€ä¹ˆæ˜¯å¤§æ¨¡å‹ï¼Ÿ

æˆ‘ä¸€ç›´ä»¥æ¥ä¹Ÿè®¤ä¸ºå¤§æ¨¡å‹å°±æ˜¯å‚æ•°é‡å¾ˆå¤§çš„æ¨¡å‹ï¼Œæ¯”å¦‚ chatgpt ï¼Œå®ƒçš„å‚æ•°é‡å°±è¾¾åˆ°äº†175B(175B)ã€‚ä½†é˜¿é‡Œé€šä¹‰åƒé—®å‘å¸ƒçš„æ¨¡å‹ Qwen-1.8B åªæœ‰18äº¿å‚æ•°ï¼Œä½†æ˜¯å®ƒçš„æ¨¡å‹ä¹Ÿè¢«ç§°ä¸ºå¤§æ¨¡å‹ã€‚é‚£ä½¿ç”¨äº†attentionçš„ç»“æ„çš„transformersç»“æ„çš„æ¨¡å‹å°±è®¤ä¸ºæ˜¯å¤§æ¨¡å‹çš„è¯ï¼Œé‚£bertä¹Ÿæ˜¯å¤§æ¨¡å‹ï¼Œæ˜¾ç„¶æˆ‘ä»¬ä¸è®¤ä¸ºbertæ˜¯å¤§æ¨¡å‹ï¼Œè€Œå›½å†…å›¢é˜Ÿç‹¬åˆ›çš„ RWKV å¤§æ¨¡å‹ï¼Œä½¿ç”¨çš„å°±æ˜¯RNNç»“æ„ã€‚æ‰€ä»¥ä¼¼ä¹ç°åœ¨æˆ‘ä»¬ä¹Ÿä¸èƒ½è§£é‡Šä»€ä¹ˆæ˜¯å¤§æ¨¡å‹ã€‚

æ‰€ä»¥å¯¹äºå¤§æ¨¡å‹çš„è§£é‡Šæˆ‘ä¹Ÿæ˜¯æ¯”è¾ƒç–‘æƒ‘çš„ã€‚å¹¸å¥½æœ‰ä¸ªä½¬å‘ Google deepmind çš„å·¨ä½¬å‘äº†é‚®ä»¶é—®äº†ä¸‹

1. å¤§æ¨¡å‹æ€ä¹ˆç®—å¤§ï¼ˆåªè¯´å‚æ•°é‡ä¸è®©æˆ‘ä¿¡æœï¼‰
2. ä¸ºä»€ä¹ˆç°åœ¨çš„å¤§æ¨¡å‹æ‰æ˜¯å¥½çš„å¤§æ¨¡å‹ 
3. å¤§å°±ä¸€å®šå¥½å—
4. å¤§è¯­è¨€æ¨¡å‹ä¸ºä»€ä¹ˆç”¨ transformeræ¶æ„ å¾ˆå°‘rnn 
5. è¯­è¨€å¤§æ¨¡å‹å’Œè§†è§‰å¤§æ¨¡å‹åŒºåˆ«åœ¨å“ª?

![Alt text](images/image-01.png)

è¿™é‡Œæ˜¯ä¸€äº›å…³äºå¤§æ¨¡å‹çš„ç»¼è¿°è®ºæ–‡ï¼Œæœ‰ Agent ä¹Ÿæœ‰ RAG ï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹å¯ä»¥å…³æ³¨ä¸‹ã€‚

å¤æ—¦å¤§æ¨¡å‹ Agent ç»¼è¿° ï¼šhttps://github.com/WooooDyy/LLM-Agent-Paper-List.git

åŒæµå¤§æ¨¡å‹ RAG ç»¼è¿° ï¼šhttps://arxiv.org/abs/2312.10997

## Windows ç¯å¢ƒä¸‹éƒ¨ç½²å¤§æ¨¡å‹

å¥½äº†ï¼Œçœ‹è¿‡ä»‹ç»ï¼Œçœ‹è¿‡è®ºæ–‡ä¹‹åå¯ä»¥ç®€å•çš„åœ¨æœ¬åœ°windowsç¯å¢ƒä¸‹ç®€å•ä½“éªŒä¸€ä¸‹å¤§æ¨¡å‹çš„é­…åŠ›ã€‚

æœ¬ç¬”è®°ä»¥ [Qwen-7b-chat](https://hf-mirror.com/Qwen/Qwen-7B-Chat) æ¨¡å‹ä¸ºåŸºç¡€è¿›è¡Œéƒ¨ç½²ï¼Œç”±äºæˆ‘æœ¬åœ°åªæœ‰ 6G æ˜¾å­˜ï¼Œæ‰€ä»¥æˆ‘å°±éƒ¨ç½²äº† [Qwen-1.8B-chat](https://hf-mirror.com/Qwen/Qwen-1_8B-Chat) æ¨¡å‹ï¼Œå¦‚æœæœ‰ 24G çš„æ˜¾å­˜ï¼Œå°±å¯ä»¥é€‰æ‹© [Qwen-7b-chat](https://hf-mirror.com/Qwen/Qwen-7B-Chat) æ¨¡å‹ã€‚

## ç¯å¢ƒé…ç½®

é¦–å…ˆæˆ‘ä»¬éœ€è¦å®‰è£… pytorch ï¼Œå‰ææ˜¯éœ€è¦å®‰è£… conda ç¯å¢ƒã€‚ä¸ºä»€ä¹ˆé€‰æ‹© conda ç¯å¢ƒï¼Ÿåœ¨conda ç¯å¢ƒä¸‹å®‰è£…torchåªéœ€è¦ä¸€è¡Œä»£ç ï¼Œè€Œä¸”å¯ä»¥ç›´æ¥åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…cudaä¾èµ–ï¼Œå¹¶ä¸”ä½¿ç”¨condaå®‰è£…cudaä¾èµ–ï¼Œåœ¨å½“è™šæ‹Ÿç¯å¢ƒä¸­å®ƒçš„ä¼˜å…ˆçº§æ˜¯æœ€é«˜çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸ä¼šå’Œç³»ç»Ÿç¯å¢ƒä¸­çš„cudaä¾èµ–å†²çªã€‚

é¦–å…ˆå®‰è£… miniconda ï¼Œä¸‹è½½åœ°å€ï¼šhttps://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/ ï¼Œé€‰æ‹©ä¸€ä¸ªå–œæ¬¢çš„ç‰ˆæœ¬ä¸‹è½½å³å¯ã€‚é€‰æ‹© miniconda æ˜¯å› ä¸º anaconda ä½“ç§¯å¤ªå¤§äº†ï¼Œå’±ä»¬ä¸éœ€è¦é‚£ä¹ˆå¤šçš„åŒ…ï¼Œåªéœ€è¦å®‰è£… miniconda å°±è¡Œäº†ã€‚ miniconda å’Œ anaconda ä½¿ç”¨èµ·æ¥æ²¡æœ‰ä»»ä½•åŒºåˆ«ã€‚

### é…ç½®é•œåƒæº

miniconda å®‰è£…å¥½äº†ä¹‹åï¼Œæˆ‘ä»¬éœ€è¦é…ç½®æ¸…åæºï¼Œå› ä¸ºæ¸…åæºçš„ä¸‹è½½é€Ÿåº¦æ¯”è¾ƒå¿«ï¼Œè€Œä¸”å›½å†…çš„ç½‘ç»œç¯å¢ƒä¹Ÿä¸æ˜¯å¾ˆå¥½ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦é…ç½®æ¸…åæºã€‚

> æ³¨æ„ï¼šæ­¤è¿‡ç¨‹éœ€è¦åœ¨Windowsçš„ Power shell ç»ˆç«¯ä¸‹è¿›è¡Œï¼Œä¸è¦ä½¿ç”¨ cmd ç»ˆç«¯ï¼Œå› ä¸º cmd ç»ˆç«¯ä¸æ”¯æŒ conda å‘½ä»¤ã€‚

- é…ç½® pip é•œåƒæº

è¿™é‡Œä½¿ç”¨çš„æ˜¯æ¸…åé•œåƒæºã€‚

```shell
# å…ˆå‡çº§ pip
python -m pip install --upgrade pip
pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple
```

- é…ç½® conda é•œåƒæº

Windows ç”¨æˆ·æ— æ³•ç›´æ¥åˆ›å»ºåä¸º `.condarc` çš„æ–‡ä»¶ï¼Œå¯å…ˆæ‰§è¡Œ `conda config --set show_channel_urls yes` ç”Ÿæˆè¯¥æ–‡ä»¶ä¹‹åå†ä¿®æ”¹ã€‚

å¯ä»¥æ‰¾åˆ°åˆšåˆšç”Ÿæˆçš„`.condarc`æ–‡ä»¶ï¼Œæ–‡ä»¶å°±åœ¨ `Windows:C:\Users\<YourUserName>\.condarc`ï¼Œåœ¨æ–‡ä»¶ä¸­æ”¾å…¥ä»¥ä¸‹å†…å®¹ï¼š

```shell
channels:
  - defaults
show_channel_urls: true
default_channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
custom_channels:
  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
```

> å¦‚æœ‰é—®é¢˜å¯ä»¥æŸ¥çœ‹é•œåƒç½‘ç«™ï¼šhttps://help.mirrors.cernet.edu.cn/

## å®‰è£… pytorch

ä½¿ç”¨ `nvidia-smi` å‘½ä»¤åœ¨ç»ˆç«¯ä¸­æŸ¥çœ‹è‡ªå·±çš„æ˜¾å¡é©±åŠ¨æ”¯æŒçš„æœ€é«˜CUDAç‰ˆæœ¬ã€‚è¿™é‡Œå¯ä»¥çœ‹åˆ°æˆ‘çš„é©±åŠ¨æœ€é«˜æ”¯æŒ12.0ç‰ˆæœ¬çš„CUDAã€‚

```shell
PS C:\Users\10213> nvidia-smi
Sun Dec 24 20:44:30 2023
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 528.49       Driver Version: 528.49       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |
| N/A   44C    P8    15W /  95W |   1446MiB /  6144MiB |      2%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
```

é¦–å…ˆåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒï¼Œè¿™é‡Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªåä¸º `nlp-llm` çš„è™šæ‹Ÿç¯å¢ƒã€‚

```shell
conda create --name nlp-llm python=3.10 -y
```

ç„¶åæ¿€æ´»ç¯å¢ƒï¼š

```shell
conda activate nlp-llm
```

æ‰“å¼€ pytorch å®˜ç½‘ï¼šhttps://pytorch.org/get-started/locally/ ï¼Œå®‰è£…torchçš„æ—¶å€™ï¼Œéœ€è¦é€‰ä¸€ä¸ªä½äºä½ ç”µè„‘æ”¯æŒçš„æœ€é«˜çš„CUDAç‰ˆæœ¬ï¼Œè¿™é‡Œæˆ‘é€‰æ‹©äº† 11.8 ç‰ˆæœ¬ã€‚

![Alt text](images/image-02.png)

ç„¶åå¤åˆ¶ä¸‹é¢çš„ä»£ç åˆ°ç»ˆç«¯ä¸­ï¼Œå°±å¯ä»¥å®‰è£… pytorch äº†ã€‚

```shell
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
```

## å®‰è£…å…¶ä»–ä¾èµ–

okï¼Œpytorchå®‰è£…å¥½äº†ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å®‰è£…å…¶ä»–çš„ä¾èµ–ï¼Œè¿™é‡Œæˆ‘ç›´æ¥ä½¿ç”¨äº†æ¸…åæºçš„é•œåƒï¼Œé€Ÿåº¦æ¯”è¾ƒå¿«ã€‚æ¯”å¦‚transformersç­‰ç­‰ã€‚

```shell
pip install modelscope==1.9.5
pip install fastapi==0.104.1
pip install uvicorn==0.24.0
pip install requests==2.25.1
pip install modelscope==1.9.5
pip install transformers==4.35.2
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.24.1
pip install transformers_stream_generator==0.0.4
```

## æ¨¡å‹ä¸‹è½½

ä½¿ç”¨ `modelscope` ä¸­çš„ `snapshot_download` å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•°`cache_dir` ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚

> ä»€ä¹ˆæ˜¯ ModelScope ï¼ŸModelScopesæ˜¯é˜¿é‡Œå·´å·´çš„ä¸€ä¸ªé¡¹ç›®ï¼Œå®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬ä¸‹è½½æ¨¡å‹ï¼ŒæŸ¥çœ‹æ¨¡å‹çš„å‚æ•°é‡ï¼Œæ¨¡å‹çš„å¤§å°ç­‰ç­‰ã€‚å®ƒçš„å®˜ç½‘åœ°å€ä¸ºï¼šhttps://www.modelscope.cn/models ï¼Œå¯ä»¥åœ¨å®˜ç½‘ä¸ŠæŸ¥çœ‹æ›´å¤šçš„æ¨¡å‹ä¿¡æ¯ã€‚modelscopeå’Œhugging faceä¸Šçš„æ¨¡å‹æ˜¯ä¸€æ ·çš„ã€‚

åœ¨ `/root/autodl-tmp` è·¯å¾„ä¸‹æ–°å»º `download.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè®°å¾—ä¿å­˜æ–‡ä»¶ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¹¶è¿è¡Œ `python download.py` æ‰§è¡Œä¸‹è½½ï¼Œæ¨¡å‹å¤§å°ä¸º `15 GB`ï¼Œä¸‹è½½æ¨¡å‹å¤§æ¦‚éœ€è¦10~20åˆ†é’Ÿ

```python
from modelscope import snapshot_download, AutoModel, AutoTokenizer
model_dir = snapshot_download('qwen/Qwen-7B-Chat', cache_dir='/root/autodl-tmp', revision='v1.1.4')
```

## web_demo è¿è¡Œ

æ–°å»º `chatBot.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè®°å¾—ä¿å­˜æ–‡ä»¶ã€‚ä¸‹é¢çš„ä»£ç æœ‰å¾ˆè¯¦ç»†çš„æ³¨é‡Šã€‚

ä¿®æ”¹ä»¥ä¸‹ä»£ç çš„ `mode_name_or_path` ã€‚ç„¶åè¿è¡Œä»£ç ï¼š

```shell
streamlit run chatBot.py
```

```python
# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import torch
import streamlit as st

# åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªé“¾æ¥
with st.sidebar:
    st.markdown("## DeepSeek LLM")
    "[å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm.git)"
    # åˆ›å»ºä¸€ä¸ªæ»‘å—ï¼Œç”¨äºé€‰æ‹©æœ€å¤§é•¿åº¦ï¼ŒèŒƒå›´åœ¨0åˆ°1024ä¹‹é—´ï¼Œé»˜è®¤å€¼ä¸º512
    max_length = st.slider("max_length", 0, 1024, 512, step=1)
    system_prompt = st.text_input("System_Prompt", "")

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Chat-Qwen")
st.caption("ğŸš€ A streamlit chatbot powered by Self-LLM")

# å®šä¹‰æ¨¡å‹è·¯å¾„
mode_name_or_path = 'your model path'

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å–tokenizer
    tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True)
    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å–æ¨¡å‹ï¼Œå¹¶è®¾ç½®æ¨¡å‹å‚æ•°
    model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, device_map='auto', torch_dtype=torch.bfloat16, trust_remote_code=True).eval()
    # Specify hyperparameters for generation
    model.generation_config = GenerationConfig.from_pretrained(mode_name_or_path, trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()  
    return tokenizer, model

# åŠ è½½Chatglm3çš„modelå’Œtokenizer
tokenizer, model = get_model()

# å¦‚æœsession_stateä¸­æ²¡æœ‰"messages"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤æ¶ˆæ¯çš„åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message("user").write(msg[0])
    st.chat_message("assistant").write(msg[1])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)
    # æ„å»ºè¾“å…¥     
    response, history = model.chat(tokenizer, prompt, system=system_prompt, history=st.session_state.messages)
    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append((prompt, response))
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)
```

![Alt text](images/image.png)

## å…³äºå¾®è°ƒ

åç»­æ›´æ–°~

å¯ä»¥å…³æ³¨ å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æ•™ç¨‹ï¼šhttps://github.com/datawhalechina/self-llm.git

## å¤§æ¨¡å‹ç›¸å…³ blog

- å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æ•™ç¨‹ï¼šhttps://github.com/datawhalechina/self-llm.git
- åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘ï¼šhttps://github.com/datawhalechina/llm-universe.git
- é¢å‘å¼€å‘è€…çš„ LLM å…¥é—¨è¯¾ç¨‹ï¼šhttps://github.com/datawhalechina/prompt-engineering-for-developers.git
- å¾®è°ƒChatGLM2-6Bè§£å†³æ–‡æœ¬äºŒåˆ†ç±»é—®é¢˜ï¼šhttps://vj6fpcxa05.feishu.cn/docx/DIged2HfIojIYlxWP9Hc2x0UnVd
- æ·±å…¥æµ…å‡º LoRAï¼šhttps://zhuanlan.zhihu.com/p/650197598
