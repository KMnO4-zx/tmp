{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-Model\n",
    "\n",
    "在transformers仓库中可以看到llama的源码，首先是LlamaModel类，继承自PreTrainedModel，这个类是所有模型的基类，包含了一些通用的方法，比如保存模型、加载模型、初始化权重等。\n",
    "\n",
    "继承关系为：`LlamaModel` -> `LlamaPreTrainedModel` -> `PreTrainedModel`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaConfig\n",
    "\n",
    "LlamaConfig 中主要是定义一些参数，比如vocab_size、hidden_size、num_hidden_layers、num_attention_heads等。所有的参数有默认值，可以直接创建cofing就能用。\n",
    "\n",
    "```python\n",
    "config = LlamaConfig()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaModel\n",
    "\n",
    "## 初始化\n",
    "\n",
    "- 设置了模型的两个属性:padding_idx（用于指定填充标记的索引），vocab_size（词汇表的大小）\n",
    "- 初始化了模型的嵌入层、解码器层、归一化层\n",
    "- 嵌入层（nn.Embedding）：模型使用嵌入层将输入的标记映射成密集的向量表示。\n",
    "- 解码器层（nn.ModuleList()）：模型包含多个解码器层，这些层都是由 LlamDecoderLayer 定义\n",
    "- 归一化层 LlamaRMSNorm：归一化层使用的是 Root Mean Square Layer Normalization（RMS Layer Norm）\n",
    "- 设置了是否使用 gradient_checkpoint 主要是用来节省显存\n",
    "- 调用 post_init() 完成一些初始化和准备检查的代码\n",
    "\n",
    "```python\n",
    "def __init__(self, config: LlamaConfig):\n",
    "    super().__init__(config)\n",
    "    self.padding_idx = config.pad_token_id\n",
    "    self.vocab_size = config.vocab_size\n",
    "\n",
    "    # embedding 层\n",
    "    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "    # 中间的一堆 decoderlayers 层\n",
    "    self.layers = nn.ModuleList(\n",
    "        [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "    )\n",
    "    self._use_sdpa = config._attn_implementation == \"sdpa\"\n",
    "    self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n",
    "    self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    self.gradient_checkpointing = False\n",
    "    # Initialize weights and apply final processing\n",
    "    self.post_init()\n",
    "```\n",
    "\n",
    "可以看一下 `post_init()` 的代码，主要是初始化权重和`gradient_checkpointing`相关的一些事情。该方法在`PreTrainedModel`基类中，`transformers`中所有模型基本都继承这个类。\n",
    "    \n",
    "```python\n",
    "def post_init(self):\n",
    "    \"\"\"\n",
    "    A method executed at the end of each Transformer model initialization, to execute code that needs the model's\n",
    "    modules properly initialized (such as weight initialization).\n",
    "    \"\"\"\n",
    "    self.init_weights()\n",
    "    self._backward_compatibility_gradient_checkpointing()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward\n",
    "\n",
    "forward 部分的代码有点长，但其实大部分都是张量并行或者是节省显存相关的代码，对于理解模型结构来说可以直接忽略。\n",
    "\n",
    "首先进来就是把 `inputs_ids` 进行向量化，然后拿到 `hidden_states` 。 然后是存起来所有的`hidden_states` 进入 `decoder_layer` 再拿一个 `hidden_states`，作为下一轮 `decoder_layer` 的 `hidden_states` 输入，最后给 `hidden_states` norm一下。 如下代码所示：\n",
    "\n",
    "```python\n",
    "inputs_embeds = self.embed_tokens(input_ids)\n",
    "hidden_states = inputs_embeds\n",
    "\n",
    "for decoder_layer in self.layers:\n",
    "    # 存起来所有的 hidden_states\n",
    "    if output_hidden_states:\n",
    "        all_hidden_states += (hidden_states,)\n",
    "    # 这里是 decoder_layer 的 forward\n",
    "    layer_outputs = decoder_layer(\n",
    "        hidden_states,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_value=past_key_values,\n",
    "        output_attentions=output_attentions,\n",
    "        use_cache=use_cache,\n",
    "    )\n",
    "    # 再拿一个 hidden_states，作为下一轮 decoder_layer 的 hidden_states 输入\n",
    "    hidden_states = layer_outputs[0]\n",
    "\n",
    "hidden_states = self.norm(hidden_states)\n",
    "```\n",
    "\n",
    "最后就是以 `BaseModelOutputWithPast` 的形式输出。ok，接下来继续看`decoder_layer`中的其他代码。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaDecoderLayer\n",
    "\n",
    "Embedding层不用多说，用的就是torch中的nn.Embedding。那就直接来看DecoderLayer。\n",
    "\n",
    "## DecoderLayers 初始化\n",
    "\n",
    "先来看初始化。\n",
    "\n",
    "- `hidden_size` : 也就是在上面说的输入输出。\n",
    "- `self_attn` : 别看它写这么多啊，其实就是选一下用什么 `attention` 。看见大写字母不要怕，直接点进去看看怎么个事！\n",
    "    ```python\n",
    "    LLAMA_ATTENTION_CLASSES = {\n",
    "        \"eager\": LlamaAttention,\n",
    "        \"flash_attention_2\": LlamaFlashAttention2,\n",
    "        \"sdpa\": LlamaSdpaAttention,\n",
    "    }\n",
    "    ```\n",
    "- `mlp` : 一个全连接层 `LlamaMLP` 这个待会后面再说，输入输出都是 `hidden_size` 大小。\n",
    "- `input_layernorm` : `LlamaRMSNorm` 层，输入时候的norm\n",
    "- `post_attention_layernorm` : 丢入 `mlp` 之前的操作。\n",
    "\n",
    "```python\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n",
    "\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\\\n",
    "```\n",
    "\n",
    "## forward\n",
    "\n",
    "首先复制一份 `hidden_states` 给 `residual`。然后 `hidden_states` 进入 `input_layernorm` 进行norm。然后进入 `self_attn` 进行 `attention` 操作，拿到 `hidden_states`、`self_attn_weights`、`present_key_value`。然后 `hidden_states` 和 `residual` 相加，得到 `hidden_states`。\n",
    "\n",
    "然后 `hidden_states` 进入 `post_attention_layernorm` 进行norm。最后 `hidden_states` 进入 `mlp` 进行全连接操作，拿到 `hidden_states`。然后 `hidden_states` 和 `residual` 相加，得到 `hidden_states`。最后输出 `hidden_states`。\n",
    "\n",
    "```python\n",
    "residual = hidden_states\n",
    "\n",
    "hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "# Self Attention\n",
    "hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "    hidden_states=hidden_states,\n",
    "    attention_mask=attention_mask,\n",
    "    position_ids=position_ids,\n",
    "    past_key_value=past_key_value,\n",
    "    output_attentions=output_attentions,\n",
    "    use_cache=use_cache,\n",
    "    **kwargs,\n",
    ")\n",
    "hidden_states = residual + hidden_states\n",
    "\n",
    "# Fully Connected\n",
    "residual = hidden_states\n",
    "hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "hidden_states = self.mlp(hidden_states)\n",
    "hidden_states = residual + hidden_states\n",
    "\n",
    "outputs = (hidden_states,)\n",
    "\n",
    "if output_attentions:\n",
    "    outputs += (self_attn_weights,)\n",
    "\n",
    "if use_cache:\n",
    "    outputs += (present_key_value,)\n",
    "\n",
    "return outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
