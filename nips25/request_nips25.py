import requests
import json
import time
import csv
from typing import List, Dict, Optional, Union
from urllib.parse import urlencode

try:
    from tqdm import tqdm
except ImportError:
    print("æç¤º: å®‰è£… tqdm åº“ä»¥è·å¾—è¿›åº¦æ¡æ˜¾ç¤º: pip install tqdm")
    # åˆ›å»ºå‡çš„ tqdm å‡½æ•°
    def tqdm(iterable, **kwargs):
        return iterable

# API é…ç½®
API_BASE_URL = "https://api2.openreview.net/notes"
LIMIT = 25  # æ¯é¡µè®ºæ–‡æ•°é‡
INITIAL_DELAY = 0.8  # åˆå§‹è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
OUTPUT_FORMAT = "json"  # è¾“å‡ºæ ¼å¼: "json" æˆ– "csv"

# å¯ä»¥ä¿®æ”¹ä¸ºä»¥ä¸‹å€¼æ¥è·å–ä¸åŒç±»å‹çš„è®ºæ–‡ï¼š
# - "NeurIPS 2025 poster" (æµ·æŠ¥è®ºæ–‡)
# - "NeurIPS 2025 oral" (å£å¤´æŠ¥å‘Šè®ºæ–‡)
# - "NeurIPS 2025 spotlight" (äº®ç‚¹è®ºæ–‡)
PAPER_VENUE = "NeurIPS 2025 spotlight"

# API è¯·æ±‚å‚æ•°é…ç½®
API_PARAMS = {
    "content.venue": PAPER_VENUE,
    "details": "replyCount,presentation,writable",
    "domain": "NeurIPS.cc/2025/Conference",
    "invitation": "NeurIPS.cc/2025/Conference/-/Submission",
    "limit": LIMIT
    # "offset" å‚æ•°ä¼šåœ¨è¯·æ±‚æ—¶åŠ¨æ€æ·»åŠ 
}

# è¯·æ±‚å¤´
HEADERS = {
    "Accept": "application/json,text/*;q=0.99",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
    "Referer": "https://openreview.net/",
    "Origin": "https://openreview.net"
}


class NIPS25Crawler:
    """NeurIPS 2025 Poster Papers Crawler"""

    def __init__(self, limit: int = 25, output_format: str = "json", delay: float = 0.8):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            limit: æ¯é¡µè·å–çš„è®ºæ–‡æ•°é‡
            output_format: è¾“å‡ºæ ¼å¼ ("json" æˆ– "csv")
            delay: APIè¯·æ±‚å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.limit = limit
        self.output_format = output_format.lower()
        self.delay = delay
        self.total_papers = 0

        # æ ¹æ® PAPER_VENUE åŠ¨æ€ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        # æå–è®ºæ–‡ç±»å‹ï¼ˆposter/oral/spotlightï¼‰å¹¶ç”¨äºæ–‡ä»¶å
        paper_type = PAPER_VENUE.split()[-1].lower()  # è·å–æœ€åä¸€ä¸ªå•è¯ï¼ˆposter/oral/spotlightï¼‰
        self.output_file = f"nips25_{paper_type}_papers.{self.output_format}"

    def construct_api_url(self, offset: int = 0) -> str:
        """
        æ„å»ºAPIè¯·æ±‚URL

        Args:
            offset: åˆ†é¡µåç§»é‡

        Returns:
            å®Œæ•´çš„API URL
        """
        # ä½¿ç”¨ API_PARAMS é…ç½®ï¼Œå¹¶åŠ¨æ€æ·»åŠ  offset å‚æ•°
        params = API_PARAMS.copy()
        params["offset"] = offset
        params["limit"] = self.limit  # ä½¿ç”¨å®ä¾‹çš„ limit å€¼
        return f"{API_BASE_URL}?{urlencode(params)}"

    def fetch_page(self, offset: int) -> Optional[Dict]:
        """
        è·å–æŒ‡å®šåˆ†é¡µçš„æ•°æ®

        Args:
            offset: åˆ†é¡µåç§»é‡

        Returns:
            APIå“åº”æ•°æ®æˆ–Noneï¼ˆè¯·æ±‚å¤±è´¥æ—¶ï¼‰
        """
        url = self.construct_api_url(offset)

        try:
            response = requests.get(url, headers=HEADERS, timeout=30)
            response.raise_for_status()

            data = response.json()

            # å¦‚æœæ˜¯ç¬¬ä¸€é¡µï¼Œè·å–æ€»æ•°
            if offset == 0:
                self.total_papers = data.get("count", 0)

            return data

        except requests.exceptions.RequestException as e:
            print(f"\nâŒ è¯·æ±‚å¤±è´¥ (offset={offset}): {e}")
            return None
        except json.JSONDecodeError as e:
            print(f"\nâŒ JSONè§£æå¤±è´¥ (offset={offset}): {e}")
            return None
        except Exception as e:
            print(f"\nâŒ æœªçŸ¥é”™è¯¯ (offset={offset}): {e}")
            return None

    def extract_paper_info(self, paper: Union[Dict, object]) -> Dict:
        """
        ä»è®ºæ–‡å¯¹è±¡ä¸­æå–æ‰€éœ€ä¿¡æ¯

        Args:
            paper: è®ºæ–‡å¯¹è±¡

        Returns:
            åŒ…å«æå–çš„ä¿¡æ¯çš„å­—å…¸
        """
        paper_data = {}

        # åŸºæœ¬ä¿¡æ¯
        paper_data["paper_id"] = paper.get("id", "")
        paper_data["forum_url"] = f"https://openreview.net/forum?id={paper.get('id', '')}"
        paper_data["number"] = paper.get("number")
        paper_data["version"] = paper.get("version")

        # æ—¶é—´æˆ³ï¼ˆè½¬æ¢ä¸ºå¯è¯»æ ¼å¼ï¼‰
        cdate_timestamp = paper.get("cdate")
        if cdate_timestamp:
            paper_data["submission_date"] = time.strftime(
                "%Y-%m-%d %H:%M:%S", time.localtime(cdate_timestamp / 1000)
            )
        else:
            paper_data["submission_date"] = ""

        # å†…å®¹å­—æ®µ
        content = paper.get("content", {})

        paper_data["title"] = content.get("title", {}).get("value", "") if isinstance(content.get("title"), dict) else content.get("title", "")

        authors = content.get("authors", {})
        if isinstance(authors, dict):
            paper_data["authors"] = authors.get("value", [])
        else:
            paper_data["authors"] = authors if isinstance(authors, list) else []

        abstract = content.get("abstract", {})
        if isinstance(abstract, dict):
            paper_data["abstract"] = abstract.get("value", "")
        else:
            paper_data["abstract"] = abstract or ""

        keywords = content.get("keywords", {})
        if isinstance(keywords, dict):
            paper_data["keywords"] = keywords.get("value", [])
        else:
            paper_data["keywords"] = keywords if isinstance(keywords, list) else []

        primary_area = content.get("primary_area", {})
        if isinstance(primary_area, dict):
            paper_data["primary_area"] = primary_area.get("value", "")
        else:
            paper_data["primary_area"] = primary_area or ""

        # PDF URL æ„å»º
        # NeurIPS 2025 çš„ PDF é“¾æ¥æœ‰ä¸¤ç§æ ¼å¼ï¼š
        # 1. https://openreview.net/pdf?id={paper_id}
        # 2. https://openreview.net/attachment?id={paper_id}&name=pdf
        # æˆ‘ä»¬ä¼˜å…ˆä½¿ç”¨ API è¿”å›çš„ pdf è·¯å¾„ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨ attachment æ ¼å¼
        pdf_path = content.get("pdf", {}).get("value", "") if isinstance(content.get("pdf"), dict) else content.get("pdf", "")
        paper_id = paper.get('id', '')

        if pdf_path.startswith("/"):
            # ä½¿ç”¨ API è¿”å›çš„å®Œæ•´è·¯å¾„
            paper_data["pdf_url"] = f"https://openreview.net{pdf_path}"
        elif paper_id:
            # ä½¿ç”¨ attachment æ ¼å¼ä½œä¸ºå¤‡é€‰: https://openreview.net/attachment?id={paper_id}&name=pdf
            paper_data["pdf_url"] = f"https://openreview.net/attachment?id={paper_id}&name=pdf"
        else:
            paper_data["pdf_url"] = ""

        # TLDR (ç®€è¦æ€»ç»“)
        tldr = content.get("TLDR", {})
        if isinstance(tldr, dict):
            paper_data["tldr"] = tldr.get("value", "")
        else:
            paper_data["tldr"] = tldr or ""

        # å›å¤æ•°é‡
        details = paper.get("details", {})
        paper_data["reply_count"] = details.get("replyCount", 0) if details else 0

        # ä¼šè®®åœºé¦†
        venue = content.get("venue", {})
        if isinstance(venue, dict):
            paper_data["venue"] = venue.get("value", "")
        else:
            paper_data["venue"] = venue or ""

        # venueid
        venueid = content.get("venueid", {})
        if isinstance(venueid, dict):
            paper_data["venueid"] = venueid.get("value", "")
        else:
            paper_data["venueid"] = venueid or ""

        return paper_data

    def process_response(self, data: Dict) -> List[Dict]:
        """
        å¤„ç†APIå“åº”æ•°æ®ï¼Œæå–è®ºæ–‡åˆ—è¡¨

        Args:
            data: APIå“åº”æ•°æ®

        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        papers = []
        notes = data.get("notes", [])

        for paper in notes:
            paper_info = self.extract_paper_info(paper)
            papers.append(paper_info)

        return papers

    def fetch_all_papers(self) -> List[Dict]:
        """
        è·å–æ‰€æœ‰è®ºæ–‡æ•°æ®

        Returns:
            æ‰€æœ‰è®ºæ–‡çš„ä¿¡æ¯åˆ—è¡¨
        """
        all_papers = []
        offset = 0
        successful_requests = 0
        failed_requests = 0

        print("=" * 60)
        print(" NeurIPS 2025 Poster Papers Crawler")
        print("=" * 60)
        print(f"è®ºæ–‡ç±»å‹: {API_PARAMS.get('content.venue')}")
        print(f"è¾“å‡ºæ ¼å¼: {self.output_format.upper()}")
        print(f"è¾“å‡ºæ–‡ä»¶: {self.output_file}")
        print("-" * 60)
        print("ğŸ” æ­£åœ¨è·å–ç¬¬ä¸€æ‰¹æ•°æ®ä»¥ç¡®å®šæ€»æ•°é‡...")

        # ç¬¬ä¸€é¡µè¯·æ±‚
        first_page = self.fetch_page(offset)
        if not first_page:
            print("âŒ æ— æ³•è·å–ç¬¬ä¸€æ‰¹æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–APIçŠ¶æ€")
            return []

        print(f"âœ… å‘ç° {self.total_papers} ç¯‡è®ºæ–‡")

        if self.total_papers == 0:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡")
            return []

        total_pages = (self.total_papers + self.limit - 1) // self.limit

        print(f"ğŸ“„ éœ€è¦è·å– {total_pages} é¡µæ•°æ® (æ¯é¡µ {self.limit} ç¯‡)")
        print("â³ å¼€å§‹è·å–æ•°æ®...")
        print("-" * 60)

        try:
            from tqdm import tqdm
            use_tqdm = True
        except ImportError:
            use_tqdm = False

        if use_tqdm:
            pbar = tqdm(total=self.total_papers, desc="è·å–è¿›åº¦", unit="paper")
        else:
            pbar = None

        current_count = 0

        # å¤„ç†ç¬¬ä¸€é¡µæ•°æ®
        papers = self.process_response(first_page)
        all_papers.extend(papers)
        successful_requests += 1
        current_count += len(papers)

        if pbar:
            pbar.update(len(papers))
        else:
            print(f"  è¿›åº¦: {current_count}/{self.total_papers} ç¯‡")

        offset += self.limit

        # å¤„ç†å‰©ä½™é¡µé¢
        while offset < self.total_papers:
            page_data = self.fetch_page(offset)

            if page_data:
                papers = self.process_response(page_data)
                all_papers.extend(papers)
                successful_requests += 1
                current_count += len(papers)

                if pbar:
                    pbar.update(len(papers))
                else:
                    print(f"  è¿›åº¦: {current_count}/{self.total_papers} ç¯‡")
            else:
                failed_requests += 1
                print(f"\nâš ï¸  è·³è¿‡ offset={offset} (è¯·æ±‚å¤±è´¥)")

            offset += self.limit
            time.sleep(min(self.delay + offset * 0.0001, 2.0))

        if pbar:
            pbar.close()

        print("-" * 60)
        print(f"âœ… æ•°æ®è·å–å®Œæˆ!")
        print(f"   - æˆåŠŸ: {successful_requests} é¡µ")
        print(f"   - å¤±è´¥: {failed_requests} é¡µ")
        print(f"   - æ€»è®¡: {len(all_papers)} / {self.total_papers} ç¯‡è®ºæ–‡")

        return all_papers

    def save_as_json(self, papers: List[Dict]) -> None:
        """
        ä¿å­˜æ•°æ®ä¸º JSON æ ¼å¼

        Args:
            papers: è®ºæ–‡æ•°æ®åˆ—è¡¨
        """
        try:
            with open(self.output_file, "w", encoding="utf-8") as f:
                json.dump(papers, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ JSON æ–‡ä»¶å·²ä¿å­˜: {self.output_file}")
            print(f"   æ–‡ä»¶å¤§å°: {len(json.dumps(papers)) / 1024:.2f} KB")
        except Exception as e:
            print(f"âŒ ä¿å­˜ JSON æ–‡ä»¶å¤±è´¥: {e}")

    def save_as_csv(self, papers: List[Dict]) -> None:
        """
        ä¿å­˜æ•°æ®ä¸º CSV æ ¼å¼

        Args:
            papers: è®ºæ–‡æ•°æ®åˆ—è¡¨
        """
        if not papers:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        try:
            fieldnames = [
                "paper_id", "number", "version", "title", "authors", "abstract",
                "keywords", "primary_area", "venue", "venueid", "tldr",
                "pdf_url", "forum_url", "submission_date", "reply_count"
            ]

            with open(self.output_file, "w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()

                for paper in papers:
                    row = paper.copy()
                    row["authors"] = "; ".join(row["authors"])
                    row["keywords"] = "; ".join(row["keywords"])
                    writer.writerow(row)

            print(f"ğŸ’¾ CSV æ–‡ä»¶å·²ä¿å­˜: {self.output_file}")

            # è®¡ç®—æ–‡ä»¶å¤§å°
            import os
            file_size = os.path.getsize(self.output_file)
            print(f"   æ–‡ä»¶å¤§å°: {file_size / 1024:.2f} KB")

        except Exception as e:
            print(f"âŒ ä¿å­˜ CSV æ–‡ä»¶å¤±è´¥: {e}")

    def save_data(self, papers: List[Dict]) -> None:
        """
        æ ¹æ®é…ç½®çš„æ ¼å¼ä¿å­˜æ•°æ®

        Args:
            papers: è®ºæ–‡æ•°æ®åˆ—è¡¨
        """
        if self.output_format == "json":
            self.save_as_json(papers)
        elif self.output_format == "csv":
            self.save_as_csv(papers)
        else:
            print(f"âš ï¸ ä¸æ”¯æŒçš„æ ¼å¼: {self.output_format}")
            print("ğŸ’¾ å°†ä½¿ç”¨ JSON æ ¼å¼ä½œä¸ºå›é€€é€‰é¡¹")
            self.output_file = "nips25_papers.json"
            self.save_as_json(papers)


def main():
    """ä¸»å‡½æ•°"""

    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = NIPS25Crawler(limit=LIMIT, output_format=OUTPUT_FORMAT, delay=INITIAL_DELAY)

        # è·å–æ‰€æœ‰è®ºæ–‡
        papers = crawler.fetch_all_papers()

        if papers:
            # ä¿å­˜æ•°æ®
            crawler.save_data(papers)
            print("-" * 60)
            print("âœ¨ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
            print("=" * 60)
        else:
            print("âš ï¸ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            return 1

    except KeyboardInterrupt:
        print("\n\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        print("âš ï¸ éƒ¨åˆ†æ•°æ®å¯èƒ½å·²è·å–ä½†æœªä¿å­˜")
        return 1
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
